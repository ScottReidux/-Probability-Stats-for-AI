{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyND2tDnLpGkWipwpKYz8Bq2",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/TAlkam/-Probability-Stats-for-AI/blob/main/CKD_data_prepration.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data Preparation and Decision Stump"
      ],
      "metadata": {
        "id": "eYMlMPXn1QBQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "import pandas as pd\n",
        "import io\n",
        "\n",
        "# Upload file\n",
        "uploaded = files.upload()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 73
        },
        "id": "1kXtqbDPX2dP",
        "outputId": "c51b2839-1635-41a7-d3ad-441f354721bf"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-f74f3b16-1039-48cc-ad78-5f5b214514b3\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-f74f3b16-1039-48cc-ad78-5f5b214514b3\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving ckd-dataset-v2 (2).csv to ckd-dataset-v2 (2).csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Steps to take:\n",
        "\n",
        "1. Importing Libraries: \n",
        "The necessary libraries and modules are imported. These include `numpy`, `pandas`, `train_test_split` from `sklearn.model_selection`, `LogisticRegression` from `sklearn.linear_model`, `confusion_matrix`, `accuracy_score` from `sklearn.metrics`, `SimpleImputer` from `sklearn.impute`, and `OneHotEncoder` from `sklearn.preprocessing`.\n",
        "\n",
        "2. Defining a Function:\n",
        "The function `process_column` is defined to handle the data preprocessing step. This function checks if a value in a column is 'discrete' or contains a '-', or is a float. It returns NaN for 'discrete', the average of the two numbers if the value contains a '-', and the float value if it's a float. If none of these conditions are met, it returns NaN.\n",
        "\n",
        "3. Loading the Dataset:\n",
        "The dataset is loaded from a CSV file using `pd.read_csv`.\n",
        "\n",
        "4. Data Preprocessing:\n",
        "The `process_column` function is applied to the necessary columns of the dataframe. The target column 'class' is converted to integer type, where 'ckd' is represented as 1 and 'notckd' as 0. Missing values in the dataframe are filled with the mean of the respective column. Then, categorical variables are one-hot encoded, and the original categorical columns are dropped from the dataframe.\n",
        "\n",
        "5. Splitting the Dataset:\n",
        "The dataset is split into features (X) and target (y). Then, it's further split into training and testing sets using `train_test_split` function.\n",
        "\n",
        "6. Data Imputation:\n",
        "A `SimpleImputer` object is created to fill any remaining missing values in the dataset with the mean of the respective column. This imputer is fit on the training data and then used to transform both training and testing data.\n",
        "\n",
        "7. Training the Model:\n",
        "A Logistic Regression model is trained using the imputed training data.\n",
        "\n",
        "8. Making Predictions:\n",
        "The model is used to make predictions on the test data.\n",
        "\n",
        "9. Evaluating the Model:\n",
        "The accuracy of the model is printed out, and a confusion matrix is displayed to evaluate the performance of the model.\n",
        "\n",
        "Note: This code is quite comprehensive and incorporates several good practices like handling missing values, converting data types, one-hot encoding categorical variables, and splitting the dataset into training and testing sets. It also makes use of logistic regression, a simple and commonly used machine learning algorithm for binary classification problems."
      ],
      "metadata": {
        "id": "th1A-Nb_amxL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Necessary imports\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeClassifier  # Import DecisionTreeClassifier instead of LogisticRegression\n",
        "from sklearn.metrics import confusion_matrix, accuracy_score\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "\n",
        "\n",
        "def process_column(col):\n",
        "    if 'discrete' in str(col):\n",
        "        return np.nan  # return NaN if 'discrete' is in column\n",
        "    if '-' in str(col):\n",
        "        low, high = map(float, str(col).split('-'))  # split on '-', convert to float\n",
        "        return (low + high) / 2  # return the average\n",
        "    else:\n",
        "        try:\n",
        "            return float(col)  # convert to float\n",
        "        except ValueError:\n",
        "            return np.nan  # if conversion to float fails, return NaN\n",
        "\n",
        "# Load the dataset\n",
        "df = pd.read_csv('ckd-dataset-v2 (2).csv')\n",
        "\n",
        "# Apply process_column function to necessary columns\n",
        "column_list = ['bp (Diastolic)', 'bp limit', 'sg', 'al', 'rbc', 'su', 'pc', 'pcc', 'ba', 'bgr', 'bu', 'sod', 'sc', 'pot', 'hemo', 'pcv', 'rbcc', 'wbcc', 'htn', 'dm', 'cad', 'appet', 'pe', 'ane', 'grf', 'stage', 'age']\n",
        "for column_name in column_list:\n",
        "    df[column_name] = df[column_name].apply(process_column)\n",
        "\n",
        "# Convert 'class' to integer type\n",
        "df['class'] = (df['class'] == 'ckd').astype(int)\n",
        "\n",
        "# Fill missing values with the mean of the respective column\n",
        "df = df.fillna(df.mean(numeric_only=True))\n",
        "\n",
        "# One-hot encode categorical variables\n",
        "enc = OneHotEncoder(drop='first')  # Create encoder object\n",
        "df_encoded = pd.DataFrame(enc.fit_transform(df.select_dtypes(include=['object'])).toarray())  # Transform data\n",
        "\n",
        "# Merge with the original df\n",
        "df = df.join(df_encoded)\n",
        "df = df.drop(df.select_dtypes(include=['object']).columns, axis=1)\n",
        "\n",
        "# Split the dataset into features and target\n",
        "X = df.drop(columns=['class'])\n",
        "y = df['class']\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Convert columns to string type to avoid issues with imputer\n",
        "X_train.columns = X_train.columns.astype(str)\n",
        "X_test.columns = X_test.columns.astype(str)\n",
        "\n",
        "# Use mean imputation\n",
        "imputer = SimpleImputer(strategy='mean')\n",
        "\n",
        "# Fit on the training data\n",
        "imputer.fit(X_train)\n",
        "\n",
        "# Transform both training and testing data\n",
        "X_train = imputer.transform(X_train)\n",
        "X_test = imputer.transform(X_test)\n",
        "\n",
        "# Train the model using the imputed training data\n",
        "model = DecisionTreeClassifier(max_depth=1)  # Replace LogisticRegression with DecisionTreeClassifier(max_depth=1)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the test data\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Print out the accuracy and confusion matrix\n",
        "print(f\"Accuracy: {accuracy_score(y_test, y_pred)}\")\n",
        "print(f\"Confusion Matrix:\\n{confusion_matrix(y_test, y_pred)}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vV77PCNbZW1z",
        "outputId": "1d19454e-f143-4eae-b667-600bc003285c"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 1.0\n",
            "Confusion Matrix:\n",
            "[[13  0]\n",
            " [ 0 28]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Logistic regression"
      ],
      "metadata": {
        "id": "6KW0PCiz1m4o"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The Logistic Regression model from the `sklearn.linear_model` module.\n",
        "\n",
        "Here's a step-by-step breakdown of what the code is doing:\n",
        "\n",
        "1. `StandardScaler()`: This creates an instance of the StandardScaler class, which will be used to standardize the features by removing the mean and scaling to unit variance. This is often a good preprocessing step for many machine learning algorithms.\n",
        "\n",
        "2. `scaler.fit_transform(X_train)`: This fits the scaler to the training data and then transforms the training data. \"Fitting\" the scaler means that it learns the parameters (mean and standard deviation for standardization) of the training data.\n",
        "\n",
        "3. `scaler.transform(X_test)`: This uses the scaler that was fitted to the training data to transform the test data. It's important to note that the same scaler is used to transform both the training and test data to ensure that they are scaled in the same way.\n",
        "\n",
        "4. `LogisticRegression(max_iter=1000)`: This creates an instance of the LogisticRegression class. The `max_iter=1000` argument sets the maximum number of iterations for the solver to converge, which can be necessary for larger datasets.\n",
        "\n",
        "5. `model.fit(X_train, y_train)`: This fits the logistic regression model to the training data. \"Fitting\" the model means that it learns the relationship between the features (X_train) and the target (y_train).\n",
        "\n",
        "6. `model.predict(X_test)`: This uses the fitted model to make predictions on the test data.\n",
        "\n",
        "7. `accuracy_score(y_test, y_pred)`: This calculates the accuracy of the model by comparing the predicted values to the actual values.\n",
        "\n",
        "So, in summary, this code is using logistic regression to make predictions on the test data and then calculating the accuracy of those predictions."
      ],
      "metadata": {
        "id": "-mMpfxjI3y4T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Scale the data\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)\n",
        "\n",
        "# Increase max_iter and fit the model\n",
        "model = LogisticRegression(max_iter=1000)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the test data\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Calculate the accuracy of the model\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Accuracy: {accuracy}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1cf06e4ZbQZb",
        "outputId": "22493066-c060-4277-fcfd-67c42a3a615f"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Linear regression"
      ],
      "metadata": {
        "id": "eE8mNpQqoi_M"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This is a linear regression model using the `LinearRegression` class from the `sklearn.linear_model` module. However, a couple of things to note:\n",
        "\n",
        "1. The target variable, 'class', is categorical in nature. Typically, for a linear regression model, the target variable would be continuous. Using a linear regression model for a binary categorical outcome could lead to predicted values outside the [0,1] range, which would not make sense for a binary classification problem. Logistic regression is generally more appropriate for binary classification problems.\n",
        "\n",
        "2. The model is trained on a single feature 'bp (Diastolic)'. While linear regression can handle multiple features, in this case, it's being used for univariate linear regression (one feature to one target).\n",
        "\n",
        "Aside from these points, the rest of the code is a standard implementation of a linear regression model:\n",
        "\n",
        "1. `LinearRegression()`: This creates an instance of the LinearRegression class.\n",
        "\n",
        "2. `linreg.fit(X_train, y_train)`: This fits the linear regression model to the training data. \"Fitting\" the model means that it learns the relationship between the features (X_train) and the target (y_train).\n",
        "\n",
        "3. `linreg.predict(X_test)`: This uses the fitted model to make predictions on the test data.\n",
        "\n",
        "4. `mean_squared_error(y_test, y_pred)`: This calculates the mean squared error of the model by comparing the predicted values to the actual values.\n",
        "\n",
        "5. `linreg.coef_` and `linreg.intercept_`: These are the parameters of the fitted linear regression model. The coefficients are the weights assigned to the features, and the intercept is the point where the fitted line crosses the y-axis when all features are 0."
      ],
      "metadata": {
        "id": "YZCOCsQod4o9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Split the dataset into features and target\n",
        "X = df[['bp (Diastolic)']]  # feature matrix\n",
        "y = df['class']  # target variable\n"
      ],
      "metadata": {
        "id": "kcoXdwjlo8mo"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert 'discrete' entries in 'age' column to NaN\n",
        "df['age'] = pd.to_numeric(df['age'], errors='coerce')\n",
        "\n",
        "# Fill NaN values with the mean of the column\n",
        "df['age'].fillna(df['age'].mean(), inplace=True)\n",
        "\n",
        "# Now you can split your data\n",
        "X = df[['age']]  # feature matrix\n",
        "y = df['class']  # target variable\n",
        "\n",
        "# And continue with the rest of your code...\n"
      ],
      "metadata": {
        "id": "FVvYb_4kpzAw"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Necessary imports\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "# Load the dataset\n",
        "df = pd.read_csv('ckd-dataset-v2 (2).csv')  # replace with your csv file\n",
        "\n",
        "# Replace non-numeric 'bp (Diastolic)' values with the median\n",
        "df['bp (Diastolic)'] = pd.to_numeric(df['bp (Diastolic)'], errors='coerce')\n",
        "df['bp (Diastolic)'].fillna(df['bp (Diastolic)'].median(), inplace=True)\n",
        "\n",
        "# Split the dataset into features and target\n",
        "X = df[['bp (Diastolic)']]  # feature matrix\n",
        "y = df['class']  # target variable\n",
        "\n",
        "# Convert categorical variable 'class' into binary indicator\n",
        "y = (y == 'ckd').astype(int)  # replace 'ckd' with the actual class label\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Create a Linear Regression object\n",
        "linreg = LinearRegression()\n",
        "\n",
        "# Train the model using the training data\n",
        "linreg.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the test data\n",
        "y_pred = linreg.predict(X_test)\n",
        "\n",
        "# Print out the mean squared error (MSE)\n",
        "print(f\"MSE: {mean_squared_error(y_test, y_pred)}\")\n",
        "\n",
        "# Print out the coefficients and the intercept\n",
        "print(f\"Coefficients: {linreg.coef_}\")\n",
        "print(f\"Intercept: {linreg.intercept_}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ecu4YFTKrO_V",
        "outputId": "ffc488be-e5c0-45a3-df2e-f11eecc55dcc"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MSE: 0.217499939801131\n",
            "Coefficients: [0.07987616]\n",
            "Intercept: 0.5789473684210525\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# *Naive Bayes:*"
      ],
      "metadata": {
        "id": "knChB59tr1d8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This script is about the Naive Bayes algorithm to a dataset for classification purposes, using the GaussianNB class from the sklearn.naive_bayes module. Here are some of the key steps:\n",
        "\n",
        "GaussianNB(): This creates an instance of the GaussianNB (Naive Bayes) class.\n",
        "\n",
        "gnb.fit(X_train, y_train): This fits the Naive Bayes model to the training data. The model learns the relationship between the features (X_train) and the target (y_train) based on the assumption that the features are independent given the target.\n",
        "\n",
        "gnb.predict(X_test): This uses the fitted model to make predictions on the test data.\n",
        "\n",
        "accuracy_score(y_test, y_pred): This computes the accuracy of the model by comparing the predicted values to the actual values.\n",
        "\n",
        "The script also includes data preprocessing steps such as filling missing values with the column mean, one-hot encoding categorical features, and checking for NaNs or non-numeric values.\n",
        "\n",
        "Note that there are some redundant lines of code (e.g., fitting the model multiple times), which may have been used for debugging or illustrating different steps. These can be cleaned up in a final version of the script."
      ],
      "metadata": {
        "id": "FTObVghzegfS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.metrics import accuracy_score\n"
      ],
      "metadata": {
        "id": "38tT2fSPr6Hl"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the dataset\n",
        "df = pd.read_csv('ckd-dataset-v2 (2).csv')\n",
        "\n",
        "# Split the dataset into features and target\n",
        "X = df.drop(columns=['class'])  # feature matrix\n",
        "y = df['class']  # target variable\n"
      ],
      "metadata": {
        "id": "upx9YaDosAxJ"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n"
      ],
      "metadata": {
        "id": "NAjJ7_T9sI8C"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Fill missing values with the mean of the respective column\n",
        "X_train = X_train.fillna(X_train.mean())\n",
        "X_test = X_test.fillna(X_train.mean())  # Use mean from training data to fill NaNs in test data\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5Aj89ilpsmID",
        "outputId": "2c0a75a0-3b9d-45e6-f97e-029cdc231214"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-39-a48c2999a58d>:2: FutureWarning: The default value of numeric_only in DataFrame.mean is deprecated. In a future version, it will default to False. In addition, specifying 'numeric_only=None' is deprecated. Select only valid columns or specify the value of numeric_only to silence this warning.\n",
            "  X_train = X_train.fillna(X_train.mean())\n",
            "<ipython-input-39-a48c2999a58d>:3: FutureWarning: The default value of numeric_only in DataFrame.mean is deprecated. In a future version, it will default to False. In addition, specifying 'numeric_only=None' is deprecated. Select only valid columns or specify the value of numeric_only to silence this warning.\n",
            "  X_test = X_test.fillna(X_train.mean())  # Use mean from training data to fill NaNs in test data\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Fill missing values with the mean of the respective column\n",
        "X_train = X_train.fillna(X_train.mean(numeric_only=True))\n",
        "X_test = X_test.fillna(X_train.mean(numeric_only=True))  # Use mean from training data to fill NaNs in test data\n"
      ],
      "metadata": {
        "id": "eRM-8W7asxat"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Fill NaNs in numeric columns with the column mean\n",
        "for col in X_train.select_dtypes(include=[np.number]).columns:\n",
        "    X_train[col] = X_train[col].fillna(X_train[col].mean())\n",
        "    X_test[col] = X_test[col].fillna(X_train[col].mean())  # Use mean from training data to fill NaNs in test data\n",
        "\n",
        "# Fill NaNs in non-numeric columns with a placeholder value\n",
        "for col in X_train.select_dtypes(include=[object]).columns:\n",
        "    X_train[col] = X_train[col].fillna('Unknown')\n",
        "    X_test[col] = X_test[col].fillna('Unknown')\n"
      ],
      "metadata": {
        "id": "AV9AlKUJtOY7"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# One-hot encode categorical features in the training data\n",
        "X_train = pd.get_dummies(X_train)\n",
        "\n",
        "# One-hot encode categorical features in the test data, ensuring it has the same columns as the training data\n",
        "X_test = pd.get_dummies(X_test)\n",
        "missing_cols = set(X_train.columns) - set(X_test.columns)\n",
        "for c in missing_cols:\n",
        "    X_test[c] = 0\n",
        "X_test = X_test[X_train.columns]\n"
      ],
      "metadata": {
        "id": "DFaLeP_itl5x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a DataFrame of zeros with the same index as X_test and columns as missing_cols\n",
        "missing_cols_df = pd.DataFrame(0, index=X_test.index, columns=list(missing_cols))\n"
      ],
      "metadata": {
        "id": "rc7zKB0puEMQ"
      },
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(X_train.isnull().sum())\n"
      ],
      "metadata": {
        "id": "SrEPbMoYuYfn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(X_train.dtypes)\n"
      ],
      "metadata": {
        "id": "uGFypHosukEH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print((X_train.values == np.inf).sum())\n",
        "print((X_train.values == -np.inf).sum())\n"
      ],
      "metadata": {
        "id": "bGe1bGNHuvh5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(X_train.dtypes)\n"
      ],
      "metadata": {
        "id": "AoWnChgYvYo_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1: Check if y_train contains NaNs or non-numeric values.\n",
        "print(\"Number of NaNs in y_train:\", y_train.isnull().sum())\n",
        "print(\"Data type of y_train:\", y_train.dtypes)\n",
        "\n",
        "# Step 2: Round the values in X_train to a certain number of decimal places.\n",
        "X_train = X_train.round(decimals=6)\n",
        "\n",
        "# Step 3: Process of elimination to find problematic columns.\n",
        "# This is just an example. You'll need to iterate over this process manually.\n",
        "for col in X_train.columns:\n",
        "    try:\n",
        "        gnb.fit(X_train[[col]], y_train)\n",
        "        print(f\"No issue with column: {col}\")\n",
        "    except ValueError:\n",
        "        print(f\"Issue with column: {col}\")\n"
      ],
      "metadata": {
        "id": "soeRhkZov36b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check the data types and NaN counts again\n",
        "print(\"Number of NaNs in y_train:\", np.isnan(y_train).sum())\n",
        "print(\"Data type of y_train:\", y_train.dtype)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n9ylX43awrQm",
        "outputId": "a976f6f6-48e4-40ca-d2ad-94113013dc78"
      },
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of NaNs in y_train: 0\n",
            "Data type of y_train: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a Gaussian Naive Bayes object\n",
        "gnb = GaussianNB()\n",
        "\n",
        "# Train the model using the training data\n",
        "gnb.fit(X_train, y_train)\n"
      ],
      "metadata": {
        "id": "EpjJfaGIsOqH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gnb = GaussianNB()\n",
        "gnb.fit(X_train, y_train)\n"
      ],
      "metadata": {
        "id": "m6Y1xugVxLZ8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Encode y_test\n",
        "y_test = le.transform(y_test)\n",
        "\n",
        "# Train the model\n",
        "gnb = GaussianNB()\n",
        "gnb.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the test data\n",
        "y_pred = gnb.predict(X_test)\n",
        "\n",
        "# Print out the accuracy\n",
        "print(f\"Accuracy: {accuracy_score(y_test, y_pred)}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M_H8JIuqy45x",
        "outputId": "fc29eb35-ca4b-48fd-8097-f32e09f89f22"
      },
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Random Forest"
      ],
      "metadata": {
        "id": "Y497m1g7fE9O"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This script is about the Random Forest algorithm to a dataset for classification purposes, using the RandomForestClassifier class from the sklearn.ensemble module. Here are the key steps:\n",
        "\n",
        "RandomForestClassifier(n_estimators=100, random_state=42): This creates an instance of the RandomForestClassifier class with 100 trees in the forest (n_estimators=100) and a specified random state for reproducibility (random_state=42).\n",
        "\n",
        "rf.fit(X_train, y_train): This fits the Random Forest model to the training data. The model learns the relationship between the features (X_train) and the target (y_train) based on an ensemble of decision trees.\n",
        "\n",
        "rf.predict(X_test): This uses the fitted model to make predictions on the test data.\n",
        "\n",
        "accuracy_score(y_test, y_pred_rf): This computes the accuracy of the model by comparing the predicted values to the actual values.\n",
        "\n",
        "The Random Forest algorithm is a type of ensemble learning method, where multiple learning algorithms are used to obtain better predictive performance. In the case of Random Forest, it builds multiple decision trees and merges them together to get a more accurate and stable prediction."
      ],
      "metadata": {
        "id": "ovU6W44we5G5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "# Initialize the Random Forest\n",
        "rf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "\n",
        "# Fit the model\n",
        "rf.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions\n",
        "y_pred_rf = rf.predict(X_test)\n",
        "\n",
        "# Print out the accuracy\n",
        "print(f\"Accuracy: {accuracy_score(y_test, y_pred_rf)}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4diymudX2eDJ",
        "outputId": "b84ce47f-49b9-4af0-c5e6-0b66f1bdf281"
      },
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 1.0\n"
          ]
        }
      ]
    }
  ]
}